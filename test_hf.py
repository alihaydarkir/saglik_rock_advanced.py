# test_ollama.py - Ollama Yerel AI Testi
import requests
import json

# Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ± adres (varsayÄ±lan olarak bu)
OLLAMA_URL = "http://localhost:11434/api/generate"

# Test sorusu
test_question = """Post CPR uygulanan hasta eÄŸer geri dÃ¶ndÃ¼yse uygulanacak ilacÄ±n adÄ± ve uygulanacak ilacÄ±n dozu kilosuna gÃ¶re nasÄ±l hesaplanmalÄ±dÄ±r? Ä°nfizyon olarak mÄ± baÅŸlatÄ±lÄ±r yoksa pushe olarak mÄ± ilaÃ§ uygulamasÄ± yapÄ±lÄ±r? Bunun deÄŸiÅŸimini hastanÄ±n entÃ¼be olup olmamasÄ± mevcut durumu deÄŸiÅŸtirir mi?"""

print("ğŸ§ª Ollama Yerel AI Testi BaÅŸlÄ±yor...")
print(f"ğŸ”— Model: llama2")
print(f"â“ Soru: {test_question[:100]}...")
print("-" * 50)

def ask_ollama(question):
    """Ollama'ya soru sor"""
    payload = {
        "model": "llama2",
        "prompt": question,
        "stream": False
    }
    
    try:
        response = requests.post(OLLAMA_URL, json=payload)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ Ollama hatasÄ±: {e}")
        return None

try:
    # Ollama'ya soru gÃ¶nder
    result = ask_ollama(test_question)
    
    if result and 'response' in result:
        print("âœ… BAÅARILI! YanÄ±t:")
        print("-" * 50)
        print(result['response'])
        print("-" * 50)
    else:
        print("âŒ YanÄ±t alÄ±namadÄ±")
        print("â„¹ï¸ Ollama Ã§alÄ±ÅŸÄ±yor mu? Terminalde 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rmayÄ± deneyin.")
        
except Exception as e:
    print(f"âŒ Beklenmeyen hata: {str(e)}")